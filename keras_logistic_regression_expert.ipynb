{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_logistic_regression_expert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNF/nPFLmudZzAFUWMpstW8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sogangori/choongang20/blob/master/keras_logistic_regression_expert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fij6UQxb5-R0",
        "colab_type": "text"
      },
      "source": [
        "공유 : http://bitly.kr/vAZP7yRR  , github.com/sogangori/choongang20\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGEdA-1b5_eH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-K2UTLN7A43",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b5554fd-5a35-410d-a997-7ad6291c95c7"
      },
      "source": [
        "x, y = load_iris(return_X_y=True)\n",
        "x.shape, y.shape, set(y) # 150개의 데이터, 클래스 k=3"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((150, 4), (150,), {0, 1, 2})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8WPWa2g7WK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()#상속한 클래스의 생성자 호출 \n",
        "    self.opt = tf.optimizers.SGD(learning_rate=0.01)#Stochatic Gradient Descent 확률적 경사 하강\n",
        "    self.dense = keras.layers.Dense(units=3, activation=keras.activations.softmax)\n",
        "  \n",
        "  def call(self, x):\n",
        "    h = self.dense(x)    \n",
        "    return h\n",
        "\n",
        "  def get_loss(self, y, h):\n",
        "    cross_entropy = - (y * tf.math.log(h) + (1 - y) * tf.math.log(1 - h)) \n",
        "    loss = tf.reduce_mean(cross_entropy)\n",
        "    return loss\n",
        "\n",
        "  def get_accuracy(self, y, h):\n",
        "    # h 3개의 확률: (m, 3), y: (m)\n",
        "    predict = tf.argmax(h, -1)\n",
        "    self.acc = tf.reduce_mean(tf.cast(tf.equal(y, predict), tf.float32)) # True > 1, False > 0    \n",
        "\n",
        "  def train(self, x, y, epoch=1):\n",
        "    # x : (m, 4), y: (m)    \n",
        "    y_hot = tf.one_hot(y, depth=3, axis=-1)#(m, 3)        \n",
        "    y_hot = tf.cast(y_hot, tf.float32)\n",
        "    for i in range(epoch):\n",
        "      with tf.GradientTape() as tape: #경사 기록 장치\n",
        "        h = self.call(x)\n",
        "        loss = self.get_loss(y_hot, h)\n",
        "      grads = tape.gradient(loss, self.trainable_variables) #경사 계산\n",
        "      self.opt.apply_gradients(zip(grads, self.trainable_variables)) # 가중치에서 경사를 빼기\n",
        "      self.get_accuracy(y, h)\n",
        "      print('%d/%d loss:%.3f acc:%.3f'%(i, epoch, loss, self.acc))\n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-1_XOwmCMX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec8f192b-fd7f-4daa-c39b-04e1ac3f9589"
      },
      "source": [
        "model.train(x, y, 100)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/100 loss:0.382 acc:0.773\n",
            "1/100 loss:0.381 acc:0.773\n",
            "2/100 loss:0.381 acc:0.773\n",
            "3/100 loss:0.381 acc:0.773\n",
            "4/100 loss:0.380 acc:0.773\n",
            "5/100 loss:0.380 acc:0.773\n",
            "6/100 loss:0.379 acc:0.773\n",
            "7/100 loss:0.379 acc:0.780\n",
            "8/100 loss:0.378 acc:0.780\n",
            "9/100 loss:0.378 acc:0.780\n",
            "10/100 loss:0.378 acc:0.780\n",
            "11/100 loss:0.377 acc:0.787\n",
            "12/100 loss:0.377 acc:0.787\n",
            "13/100 loss:0.376 acc:0.787\n",
            "14/100 loss:0.376 acc:0.787\n",
            "15/100 loss:0.376 acc:0.787\n",
            "16/100 loss:0.375 acc:0.787\n",
            "17/100 loss:0.375 acc:0.787\n",
            "18/100 loss:0.374 acc:0.787\n",
            "19/100 loss:0.374 acc:0.787\n",
            "20/100 loss:0.374 acc:0.787\n",
            "21/100 loss:0.373 acc:0.793\n",
            "22/100 loss:0.373 acc:0.793\n",
            "23/100 loss:0.372 acc:0.793\n",
            "24/100 loss:0.372 acc:0.793\n",
            "25/100 loss:0.372 acc:0.793\n",
            "26/100 loss:0.371 acc:0.793\n",
            "27/100 loss:0.371 acc:0.793\n",
            "28/100 loss:0.371 acc:0.793\n",
            "29/100 loss:0.370 acc:0.793\n",
            "30/100 loss:0.370 acc:0.793\n",
            "31/100 loss:0.369 acc:0.793\n",
            "32/100 loss:0.369 acc:0.793\n",
            "33/100 loss:0.369 acc:0.793\n",
            "34/100 loss:0.368 acc:0.800\n",
            "35/100 loss:0.368 acc:0.800\n",
            "36/100 loss:0.368 acc:0.800\n",
            "37/100 loss:0.367 acc:0.800\n",
            "38/100 loss:0.367 acc:0.800\n",
            "39/100 loss:0.367 acc:0.800\n",
            "40/100 loss:0.366 acc:0.800\n",
            "41/100 loss:0.366 acc:0.800\n",
            "42/100 loss:0.366 acc:0.800\n",
            "43/100 loss:0.365 acc:0.800\n",
            "44/100 loss:0.365 acc:0.800\n",
            "45/100 loss:0.365 acc:0.800\n",
            "46/100 loss:0.364 acc:0.800\n",
            "47/100 loss:0.364 acc:0.800\n",
            "48/100 loss:0.364 acc:0.800\n",
            "49/100 loss:0.363 acc:0.800\n",
            "50/100 loss:0.363 acc:0.800\n",
            "51/100 loss:0.363 acc:0.800\n",
            "52/100 loss:0.362 acc:0.800\n",
            "53/100 loss:0.362 acc:0.800\n",
            "54/100 loss:0.362 acc:0.800\n",
            "55/100 loss:0.361 acc:0.800\n",
            "56/100 loss:0.361 acc:0.800\n",
            "57/100 loss:0.361 acc:0.800\n",
            "58/100 loss:0.360 acc:0.800\n",
            "59/100 loss:0.360 acc:0.800\n",
            "60/100 loss:0.360 acc:0.800\n",
            "61/100 loss:0.359 acc:0.800\n",
            "62/100 loss:0.359 acc:0.800\n",
            "63/100 loss:0.359 acc:0.800\n",
            "64/100 loss:0.358 acc:0.800\n",
            "65/100 loss:0.358 acc:0.800\n",
            "66/100 loss:0.358 acc:0.800\n",
            "67/100 loss:0.357 acc:0.800\n",
            "68/100 loss:0.357 acc:0.800\n",
            "69/100 loss:0.357 acc:0.800\n",
            "70/100 loss:0.357 acc:0.800\n",
            "71/100 loss:0.356 acc:0.800\n",
            "72/100 loss:0.356 acc:0.800\n",
            "73/100 loss:0.356 acc:0.800\n",
            "74/100 loss:0.355 acc:0.800\n",
            "75/100 loss:0.355 acc:0.800\n",
            "76/100 loss:0.355 acc:0.800\n",
            "77/100 loss:0.354 acc:0.800\n",
            "78/100 loss:0.354 acc:0.800\n",
            "79/100 loss:0.354 acc:0.800\n",
            "80/100 loss:0.354 acc:0.807\n",
            "81/100 loss:0.353 acc:0.807\n",
            "82/100 loss:0.353 acc:0.807\n",
            "83/100 loss:0.353 acc:0.807\n",
            "84/100 loss:0.352 acc:0.807\n",
            "85/100 loss:0.352 acc:0.807\n",
            "86/100 loss:0.352 acc:0.807\n",
            "87/100 loss:0.352 acc:0.807\n",
            "88/100 loss:0.351 acc:0.807\n",
            "89/100 loss:0.351 acc:0.807\n",
            "90/100 loss:0.351 acc:0.807\n",
            "91/100 loss:0.350 acc:0.807\n",
            "92/100 loss:0.350 acc:0.807\n",
            "93/100 loss:0.350 acc:0.807\n",
            "94/100 loss:0.350 acc:0.807\n",
            "95/100 loss:0.349 acc:0.807\n",
            "96/100 loss:0.349 acc:0.813\n",
            "97/100 loss:0.349 acc:0.813\n",
            "98/100 loss:0.349 acc:0.813\n",
            "99/100 loss:0.348 acc:0.813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8sSRZpOGL8G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c39fd1e3-1d3d-4a86-b9e1-a428d2020cdf"
      },
      "source": [
        "h = model(x[:1])\n",
        "print(np.array(h)) #확률"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.7783411  0.18895338 0.03270547]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmr_GImWCDqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}